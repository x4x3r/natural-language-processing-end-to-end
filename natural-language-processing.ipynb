{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b6201e9e",
   "metadata": {},
   "source": [
    "# Task level 1 (mandatory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd4f98bf",
   "metadata": {},
   "source": [
    "### What is your approach to the problem?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5362f612",
   "metadata": {},
   "source": [
    "My approach would be data extraction, cleaning and transforming to prepapare the data for the final machine learning task using natural language processing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5362e905",
   "metadata": {},
   "source": [
    "### How will you source data for your purposes?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ddda0d2",
   "metadata": {},
   "source": [
    "First, I will collect the data from the top 20 websites by each of the mentioned verticals (Sports, TV Movies and Streaming, File Sharing and Hosting). Depending in the project size, I would use BeautifulSoup or Scrapy for this task. I would consider using Selenium Webdriver for dynamic webpages. The scraper will collect the keywords from the meta tags and will create a two-column csv file. The first will hold the feature vector and the second - the class (dependent) variable. I intend to create this dataset while scraping.\n",
    "\n",
    "Next, I will transform the data to match input format required by the machine learning model. I might use Python Pandas in this step. Stemming and stopwords removal mightimprove the model performance. This will create the training dataset.\n",
    "\n",
    "Last, I will train a model on the training data and apply the model on the test data. I will use I Naive Bayes classifier for batch processing or some streaming library (River, Scikit Multiflow) for stream processing. The formed are useful when the data have already been downloaded and process, while the latter can be used after we have developed a machine learning workflow."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "856495de",
   "metadata": {},
   "source": [
    "### What are edge cases and considerations to take into account?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9ffe18b",
   "metadata": {},
   "source": [
    "Some websites might have not have data in the meta tags. In this case, the data can be extracted as plain text from the website text. This requires additional processing for keywords and keyphrase extraction. The nltk library can be useful in this context.\n",
    "\n",
    "Some websites might block the scraper. This is solved with setting up the user agent headers and delaying consecutive requests.\n",
    "\n",
    "Some websites my not load, because they do not exist or are not accessible from my location. If websites do not exists, Python exceptions can be used to continue scraping the remaining sites. If not accessible, free or paid proxy servers might help preventing geo-blocking.\n",
    "\n",
    "Website geolocation has caused some webistes to return information in Bulgarian. Sometimes information is useful and can be translated with Google Translate API.\n",
    "\n",
    "Some websites might be in languages other than English. Google Translate API can be used to detect the language of the document and translate text to English. Then nltk can be used for keyword extraction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6345cae7",
   "metadata": {},
   "source": [
    "# Task level 2 (optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48dcb106",
   "metadata": {},
   "source": [
    "1. Import the required data and libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "56355107",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://www.similarweb.com/top-websites/sports/',\n",
       " 'https://www.similarweb.com/top-websites/arts-and-entertainment/tv-movies-and-streaming/',\n",
       " 'https://www.similarweb.com/top-websites/computers-electronics-and-technology/file-sharing-and-hosting/']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from urllib.error import HTTPError, URLError\n",
    "# from selenium import webdriver\n",
    "import pandas as pd\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import time\n",
    "import re\n",
    "\n",
    "# Import the start urls from a file\n",
    "with open(\"website_links.txt\", \"r\") as f:\n",
    "    start_urls = []\n",
    "    for line in f:\n",
    "        start_urls.append(line.strip())\n",
    "\n",
    "# or read them from a list\n",
    "# start_urls = [\"https://www.similarweb.com/top-websites/category/sports/\",\n",
    "#               \"https://www.similarweb.com/top-websites/category/arts-and-entertainment/tv-movies-and-streaming/\",\n",
    "#               \"https://www.similarweb.com/top-websites/category/computers-electronics-and-technology/file-sharing-and-hosting/\"]\n",
    "start_urls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ac53d91",
   "metadata": {},
   "source": [
    "2. Scrape data from Similarweb ranking tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbed3e49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_similarweb_data(urls):\n",
    "    \"\"\"scrape Similarweb\"\"\"\n",
    "    headers = {'User-Agent':'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:99.0) Gecko/20100101 Firefox/99.0',\n",
    "          'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8',\n",
    "           'Accept-Language': 'en-US,en;q=0.5',\n",
    "           'Connection': 'keep-alive'}\n",
    "        \n",
    "    similarweb_data = []\n",
    "    \n",
    "    for url in urls:\n",
    "        response = requests.get(url, headers=headers)\n",
    "        bs = BeautifulSoup(response.text, \"html.parser\")\n",
    "        verticals =  url.rsplit(\"/\", -1)[-2]\n",
    "        \n",
    "        for row in bs.find(\"tbody\").find_all(\"tr\"):\n",
    "            data_points = row.find_all(\"td\")\n",
    "            similarweb_data.append({\n",
    "                \"rank\": data_points[0].get_text().strip(),\n",
    "                \"website\": data_points[1].get_text().strip(),\n",
    "                \"category\": data_points[2].get_text().strip(),\n",
    "                \"change\": data_points[3].get_text().strip(),\n",
    "                \"avg_visit_duration\": data_points[4].get_text().strip(),\n",
    "                \"pages_per_visit\": data_points[5].get_text().strip(),\n",
    "                \"bounce_rate\": data_points[6].get_text().strip(),\n",
    "                \"vertical\": verticals\n",
    "            })\n",
    "            \n",
    "    return similarweb_data    \n",
    "\n",
    "# Convert this to Pandas data frame for further analysis\n",
    "d = get_similarweb_data(start_urls)\n",
    "df = pd.DataFrame.from_dict(d, orient='columns')\n",
    "df.set_index(keys=df[\"rank\"], inplace=True)\n",
    "df.drop(\"rank\", axis=1, inplace=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72b5ed94",
   "metadata": {},
   "source": [
    "**2.1. Another web scraping function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9541666e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_similarweb_data(urls):\n",
    "    \"\"\"scrape Similarweb table\"\"\"\n",
    "    \n",
    "    headers = {'User-Agent':'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:99.0) Gecko/20100101 Firefox/99.0',\n",
    "          'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8',\n",
    "           'Accept-Language': 'en-US,en;q=0.5',\n",
    "           'Connection': 'keep-alive'}\n",
    "    \n",
    "    similarweb_data = []\n",
    "    \n",
    "#     with open('/home/user/projects/adcash_interview_v1/interview-homework-v1/soup.html') as fp:\n",
    "#         soup = BeautifulSoup(fp, 'html.parser')\n",
    "    \n",
    "    for url in urls:\n",
    "        response = requests.get(url, headers=headers)\n",
    "        bs = BeautifulSoup(response.text, \"html.parser\")\n",
    "        verticals =  url.rsplit(\"/\", -1)[-2]\n",
    "    \n",
    "        for row in bs.find('.tw-table div.data-section__content').find_all('div.tw-table__row'):\n",
    "            similarweb_data.append({\n",
    "                \"rank\": row.find('span.tw-table__row-rank').get_text().strip(),\n",
    "                \"website\": row.find('span.tw-table__row-domain').get_text().strip(),\n",
    "                \"category\": row.find('span.tw-table__row-category').get_text().strip(),\n",
    "                \"change\": row.find('span.tw-table__row-rank-change').get_text().strip(),\n",
    "                \"avg_visit_duration\": row.find('span.tw-table__row-avg-visit-duration').get_text().strip(),\n",
    "                \"pages_per_visit\": row.find('span.tw-table__row-pages-per-visit').get_text().strip(),\n",
    "                \"bounce_rate\": row.find('span.tw-table__row-bounce-rate').get_text().strip(),\n",
    "                \"vertical\": verticals\n",
    "            })\n",
    "    \n",
    "    return similarweb_data\n",
    "\n",
    "# Convert this to Pandas data frame for further analysis\n",
    "data = get_similarweb_data(start_urls)\n",
    "print(data)\n",
    "# df = pd.DataFrame.from_dict(d, orient='columns')\n",
    "# df.set_index(keys=df[\"rank\"], inplace=True)\n",
    "# df.drop(\"rank\", axis=1, inplace=True)\n",
    "# df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "816619d9",
   "metadata": {},
   "source": [
    "**2.2. Scraping with Selenium web driver**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1416a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_similarweb_data(urls):\n",
    "    \"\"\"scrape Similarweb new div table\"\"\"\n",
    "    \n",
    "    headers = {'User-Agent':'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:99.0) Gecko/20100101 Firefox/99.0',\n",
    "          'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8',\n",
    "           'Accept-Language': 'en-US,en;q=0.5',\n",
    "           'Connection': 'keep-alive'}\n",
    "    \n",
    "    similarweb_data = []\n",
    "    \n",
    "    for url in urls:\n",
    "        driver = webdriver.Firefox()\n",
    "        driver.get(url)\n",
    "        bs = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "        driver.quit()\n",
    "        verticals =  url.rsplit(\"/\", -1)[-2]\n",
    "    \n",
    "        for row in bs.select('.tw-table div.data-section__content div.tw-table__row'):\n",
    "            similarweb_data.append({\n",
    "                \"rank\": row.find('span.tw-table__row-rank').get_text().strip(),\n",
    "                \"website\": row.find('span.tw-table__row-domain').get_text().strip(),\n",
    "                \"category\": row.find('span.tw-table__row-category').get_text().strip(),\n",
    "                \"change\": row.find('span.tw-table__row-rank-change').get_text().strip(),\n",
    "                \"avg_visit_duration\": row.find('span.tw-table__row-avg-visit-duration').get_text().strip(),\n",
    "                \"pages_per_visit\": row.find('span.tw-table__row-pages-per-visit').get_text().strip(),\n",
    "                \"bounce_rate\": row.find('span.tw-table__row-bounce-rate').get_text().strip(),\n",
    "                \"vertical\": verticals\n",
    "            })\n",
    "    \n",
    "    return similarweb_data\n",
    "\n",
    "# Convert this to Pandas data frame for further analysis\n",
    "data = get_similarweb_data(start_urls)\n",
    "df = pd.DataFrame.from_dict(data, orient='columns')\n",
    "df.set_index(keys=df[\"rank\"], inplace=True)\n",
    "df.drop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba0f2f33",
   "metadata": {},
   "source": [
    "**2.3 Scraping using pandas.read_html() method for extracting data from HTML table**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12a93640",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternatively, scrape Similarweb with Pandas\n",
    "\n",
    "def get_similarweb_data(urls):\n",
    "    \n",
    "    headers = {'User-Agent':'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:99.0) Gecko/20100101 Firefox/99.0',\n",
    "          'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8',\n",
    "           'Accept-Language': 'en-US,en;q=0.5',\n",
    "           'Connection': 'keep-alive'}\n",
    "    \n",
    "    df_list = []\n",
    "    \n",
    "    for url in urls:\n",
    "        response = requests.get(url=url, headers=headers)\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")     \n",
    "        dfs = pd.read_html(str(soup), header=0)\n",
    "        vertical =  url.rsplit(\"/\", -1)[-2]\n",
    "        df = pd.concat(dfs)\n",
    "        df['vertical'] = vertical\n",
    "        df_list.append(df)\n",
    "        df = pd.concat(df_list) # concatenate (union) of 3 dataframes\n",
    "        df.columns = ['rank','website','category','change','avg_visit_duration',\n",
    "                      'pages_per_visit','bounce_rate', 'vertical']\n",
    "        df.set_index(df['rank'], inplace=True, drop=True)\n",
    "        df.drop(\"rank\", axis=1, inplace=True)\n",
    "  \n",
    "    return df\n",
    "\n",
    "get_similarweb_data(start_urls)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b77a7ce6",
   "metadata": {},
   "source": [
    "**2.4. One more option is to scrape and store the data in Python dictionary, and then convert to pandas.DataFrame**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66a90bf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_similarweb_data(urls):\n",
    "    \"\"\"scrape Similarweb\"\"\"\n",
    "    \n",
    "    headers = {'User-Agent':'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:99.0) Gecko/20100101 Firefox/99.0',\n",
    "          'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8',\n",
    "           'Accept-Language': 'en-US,en;q=0.5',\n",
    "           'Connection': 'keep-alive'}\n",
    "        \n",
    "    website_data = {}\n",
    "    \n",
    "    for url in urls:\n",
    "        response = requests.get(url, headers=headers)\n",
    "        bs = BeautifulSoup(response.text, \"html.parser\")\n",
    "        vertical =  url.rsplit(\"/\", -1)[-2]\n",
    "        \n",
    "        for row in bs.find(\"tbody\").find_all(\"tr\"):\n",
    "            data_points = row.find_all(\"td\")\n",
    "            website_data.setdefault(vertical, [])\n",
    "            website_data[vertical].append({\n",
    "                \"rank\": data_points[0].get_text().strip(),\n",
    "                \"website\": data_points[1].get_text().strip(),\n",
    "                \"category\": data_points[2].get_text().strip(),\n",
    "                \"change\": data_points[3].get_text().strip(),\n",
    "                \"avg_visit_duration\": data_points[4].get_text().strip(),\n",
    "                \"pages_per_visit\": data_points[5].get_text().strip(),\n",
    "                \"bounce_rate\": data_points[6].get_text().strip(),\n",
    "                \"vertical\": vertical\n",
    "            })\n",
    "            \n",
    "    return website_data    \n",
    "    \n",
    "data = get_similarweb_data(start_urls)\n",
    "data\n",
    "# df = pd.concat({k: pd.DataFrame(v).set_index('rank') for k, v in data.items()}, sort=False)\n",
    "sports_df = pd.DataFrame(data[\"sports\"])\n",
    "tv_movies_df = pd.DataFrame(data[\"tv-movies-and-streaming\"])\n",
    "file_sharing_df = pd.DataFrame(data[\"file-sharing-and-hosting\"])\n",
    "df = pd.concat([sports_df, tv_movies_df, file_sharing_df], sort=False).set_index(\"rank\")\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12164dc6",
   "metadata": {},
   "source": [
    "There are several ways that this code could be improved. Here are a few suggestions:\n",
    "\n",
    "- Instead of using a series of if statements to check which meta tag has some text, you could use a dictionary to map the different meta tag names to their corresponding values. This would make the code more readable and easier to maintain.\n",
    "\n",
    "- The code currently catches several different types of exceptions, but it does not handle them in a consistent way. Some exceptions are printed to the console, while others are silently ignored. It would be better to handle all exceptions in the same way, such as by logging the error and appending an empty string to the keywords_list.\n",
    "\n",
    "- The code makes a request to each website in the df.website data frame and then parses the response using BeautifulSoup. This is a slow and resource-intensive way to process the data. Instead, you could use the requests-futures library to make requests to all of the websites in parallel, which would significantly improve the performance of the code.\n",
    "\n",
    "- The code uses a timeout parameter when making requests to websites, but it is set to a very low value of 2 seconds. This may not be sufficient time for some websites to respond, and if the request times out, the code will raise an exception. It would be better to set the timeout to a higher value, such as 10 seconds, to give the websites more time to respond.\n",
    "\n",
    "- Finally, the code does not check whether the response from a website was successful (e.g. with a status code of 200). If a website returns an error code, the code will still attempt to parse the response and append the result to the keywords_list, which may not be what you want. It would be better to check the status code of the response and only append the result to the keywords_list if the response was successful."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45db7e23",
   "metadata": {},
   "source": [
    "**3. Scraping the websites' metadata and text**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceac7f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scrape meta tags and webpage text from each of the 150 websites\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "def extract_keywords(url):\n",
    "    headers = {'User-Agent':'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:99.0) Gecko/20100101 Firefox/99.0',\n",
    "              'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8',\n",
    "               'Accept-Language': 'en-US,en;q=0.5',\n",
    "               'Connection': 'keep-alive'}\n",
    "    \n",
    "    keywords_list = []\n",
    "    \n",
    "    # Use ThreadPoolExecutor to make requests in parallel\n",
    "    with ThreadPoolExecutor() as executor:\n",
    "    # Get a list of futures for each request\n",
    "        futures = [executor.submit(requests.get,\n",
    "                                   url='https://' + url,\n",
    "                                   headers=headers,\n",
    "                                   timeout=5) for url in df.website]\n",
    "    \n",
    "        # Iterate over the futures and extract the keywords\n",
    "        for future in futures:\n",
    "            try:\n",
    "                # Wait for the future to complete and get the response\n",
    "                response = future.result()\n",
    "                soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "                \n",
    "                # Extract html meta tag data and page text\n",
    "                meta_tags = soup.find_all(\"meta\")\n",
    "                for meta_tag in meta_tags:\n",
    "                    meta_content = meta_tag.get(\"content\")\n",
    "                page_text = soup.get_text(separator='.')\n",
    "                \n",
    "                # Append meta_tags or page_text if not empty, else append empty string\n",
    "                if meta_content or page_text:\n",
    "                    # concatenate the meta tags and page text into a single string\n",
    "                    keywords = \"|\".join([str(meta_content), page_text])\n",
    "                    keywords_list.append(keywords)\n",
    "                else:\n",
    "                    keywords_list.append('')\n",
    "            \n",
    "            except (URLError, HTTPError, requests.exceptions.ConnectionError, requests.exceptions.ReadTimeout) as e:\n",
    "                # Handle any errors that occur\n",
    "                keywords_list.append(\"\")\n",
    "                print(e)\n",
    "                \n",
    "    \n",
    "    return keywords_list\n",
    "\n",
    "# Extract data from the top 20 websites in the data frame\n",
    "df[\"keywords\"] = extract_keywords(df.website)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7342ec3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Storing the scraped data to csv for further processing (or if power runs out)\n",
    "df.to_csv('scraped_data_v3.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99b3fe0e",
   "metadata": {},
   "source": [
    "### Data processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "128e9d30",
   "metadata": {},
   "source": [
    "Proceed with data cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a01c75c8",
   "metadata": {},
   "source": [
    "**1. Create initial training data (X) and test data (y).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "34c10c87",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Either, read in the scraped data stored as csv file on the hard disk.\n",
    "# Subset the columns that will be used for training.\n",
    "X = pd.read_csv(\"scraped_data_v3.csv\", usecols=['keywords', 'vertical'])[['keywords', 'vertical']].dropna()\n",
    "\n",
    "# Or, created the training data by subsetting the training data from the existing df\n",
    "# X = df[['Keywords', 'Vertical']]\n",
    "\n",
    "# Read the test data\n",
    "y = pd.read_csv('test-keyword-samples', names=['keywords', 'vertical'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "17eb38cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>keywords</th>\n",
       "      <th>vertical</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>400|\\n. .Live Cricket Score, Schedule, Latest ...</td>\n",
       "      <td>sports</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>index|\\n.\\n.\\n.\\n.\\n.\\n.\\n.\\n.\\n.ESPN - Servin...</td>\n",
       "      <td>sports</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1| .\\n.\\n.\\n.\\n.\\n.\\n.\\n.MARCA - Diario online...</td>\n",
       "      <td>sports</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>78|Today's Cricket Match | Cricket Update | Cr...</td>\n",
       "      <td>sports</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Finetwork Liga F 2022/2023|AS.com - Diario onl...</td>\n",
       "      <td>sports</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            keywords vertical\n",
       "0  400|\\n. .Live Cricket Score, Schedule, Latest ...   sports\n",
       "1  index|\\n.\\n.\\n.\\n.\\n.\\n.\\n.\\n.\\n.ESPN - Servin...   sports\n",
       "2  1| .\\n.\\n.\\n.\\n.\\n.\\n.\\n.MARCA - Diario online...   sports\n",
       "3  78|Today's Cricket Match | Cricket Update | Cr...   sports\n",
       "4  Finetwork Liga F 2022/2023|AS.com - Diario onl...   sports"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "da6f2649",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>keywords</th>\n",
       "      <th>vertical</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>watch series online,watchseries,watch series,v...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>anime online,anime online sub español,anime on...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>football stream,nfl stream,soccer stream,tenni...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>football footem.site,footem7,footem.site,epics...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>online storage, free storage, cloud Storage, c...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            keywords  vertical\n",
       "0  watch series online,watchseries,watch series,v...       NaN\n",
       "1  anime online,anime online sub español,anime on...       NaN\n",
       "2  football stream,nfl stream,soccer stream,tenni...       NaN\n",
       "3  football footem.site,footem7,footem.site,epics...       NaN\n",
       "4  online storage, free storage, cloud Storage, c...       NaN"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac285bdd",
   "metadata": {},
   "source": [
    "**2. Clean up the data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e0622d66",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/user/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "\n",
    "try:\n",
    "    # Check if the wordnet resource is already downloaded\n",
    "    nltk.data.find('corpora/wordnet')\n",
    "except LookupError:\n",
    "    # Download the wordnet resource if it's not found\n",
    "    nltk.download('wordnet')\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"Remove whitespace, single and special characters\"\"\"\n",
    "    \n",
    "    # Remove all the special characters\n",
    "    text = re.sub(r'\\W', ' ', str(text))\n",
    "\n",
    "    # remove all single characters\n",
    "    text = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', str(text))\n",
    "\n",
    "    # Remove single characters from the start\n",
    "    text = re.sub(r'\\^[a-zA-Z]\\s+', ' ', str(text))\n",
    "\n",
    "    # Substituting multiple spaces with single space\n",
    "    text = re.sub(r'\\s+', ',', str(text))\n",
    "\n",
    "    # Removing prefixed 'b'\n",
    "    text = re.sub(r'^b\\s+', '', str(text))\n",
    "\n",
    "    # Converting to Lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    return text\n",
    "\n",
    "# Lemmatization\n",
    "def lemmatize_text(text):\n",
    "    \"\"\"Stemming keywords\"\"\"\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    return lemmatizer.lemmatize(text)\n",
    "\n",
    "# Apply the clean_text() and lemmatize text() on training data\n",
    "X['keywords'] = X['keywords'].apply(clean_text)\n",
    "X['keywords'] = X['keywords'].apply(lemmatize_text)\n",
    "\n",
    "# Apply the clean_text() and lemmatize text() on test data\n",
    "y['keywords'] = y['keywords'].apply(clean_text)\n",
    "y['keywords'] = y['keywords'].apply(lemmatize_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1189924",
   "metadata": {},
   "source": [
    "We first apply `CountVectorizer` to convert the text data into a matrix where each row represents a document and each column represents a unique word (or token) in the corpus. The cells of the matrix contain the counts of how many times each word appears in each document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "656951a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word tokenization\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "count_vect = CountVectorizer()\n",
    "X_train_counts = count_vect.fit_transform(X['keywords'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9be28239",
   "metadata": {},
   "source": [
    "We then apply TF-IDF (“Term Frequency times Inverse Document Frequency”), which uses word frequnecy to determine word importance to a given document. The algorithm assigns more weght to words that appear frequently in a document but rarely in other documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28c7a9af",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# TF-IDF transformation on the training set\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
    "X_train_tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f1e80df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF transformation on the test set\n",
    "\n",
    "y_test_counts = count_vect.transform(y['keywords'])\n",
    "y_test_tfidf = tfidf_transformer.transform(y_test_counts)\n",
    "y_test_tfidf.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08e1ddb3",
   "metadata": {},
   "source": [
    "However, instead of applying `CountVectorizer` and `TfidfTransformer` separaterly, we can use `TfidfTransformer`, which combines both word tokenization and TF-IDF transformation in one single step:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6838e46e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6, 36514)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# Fit TfidfVectorizer on the training data\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X['keywords'])\n",
    "X_train_tfidf.shape\n",
    "\n",
    "# Transform the test data using the fitted vectorizer\n",
    "y_test_tfidf = tfidf_vectorizer.transform(y['keywords'])\n",
    "y_test_tfidf.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2d5e27a",
   "metadata": {},
   "source": [
    "### Model selection and optimization\n",
    "We need to select the best performing classification algorithm/model and to perform hyperparameter optimization on the selected model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da3bb273",
   "metadata": {},
   "source": [
    "### Apply machine learning model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "123961f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# Train on scraped data\n",
    "clf = MultinomialNB().fit(X_train_tfidf, X['vertical'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1e0ba738",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply trained model on the test data (file: test-keyword-samples)\n",
    "predicted = clf.predict(y_test_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "66c6761b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add predictions to y['vertical']\n",
    "y['vertical'] = clf.predict(y_test_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4923adef",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>keywords</th>\n",
       "      <th>vertical</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>watch,series,online,watchseries,watch,series,v...</td>\n",
       "      <td>tv-movies-and-streaming</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>anime,online,anime,online,sub,español,anime,on...</td>\n",
       "      <td>tv-movies-and-streaming</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>football,stream,nfl,stream,soccer,stream,tenni...</td>\n",
       "      <td>sports</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>football,footem,site,footem7,footem,site,epics...</td>\n",
       "      <td>sports</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>online,storage,free,storage,cloud,storage,coll...</td>\n",
       "      <td>file-sharing-and-hosting</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>your,videos,easily,higher,advertisingrevenues,...</td>\n",
       "      <td>file-sharing-and-hosting</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            keywords                  vertical\n",
       "0  watch,series,online,watchseries,watch,series,v...   tv-movies-and-streaming\n",
       "1  anime,online,anime,online,sub,español,anime,on...   tv-movies-and-streaming\n",
       "2  football,stream,nfl,stream,soccer,stream,tenni...                    sports\n",
       "3  football,footem,site,footem7,footem,site,epics...                    sports\n",
       "4  online,storage,free,storage,cloud,storage,coll...  file-sharing-and-hosting\n",
       "5  your,videos,easily,higher,advertisingrevenues,...  file-sharing-and-hosting"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.to_csv(\"MultinomialNB_predictions.csv\", header=True)\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2464fb5",
   "metadata": {},
   "source": [
    "Even without evalutation of the model performance on the test data it is obvious that the model predicted the class in the column vertical quite well"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b8e8ac4",
   "metadata": {},
   "source": [
    "## Model selection and evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c349a7c",
   "metadata": {},
   "source": [
    "Although Multinomial Naive Bayes model returned perfect predictions, there are other models that can be used for such NLP tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "682afdb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultinomialNB: Validation Accuracy = 0.6207\n",
      "Logistic Regression: Validation Accuracy = 0.7241\n",
      "RandomForestClassifier: Validation Accuracy = 0.8276\n",
      "Best Model: RandomForestClassifier with Validation Accuracy = 0.8276\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Split the data into train, validation and test sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_tfidf, X['vertical'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the models to evaluate\n",
    "models = [\n",
    "    (\"MultinomialNB\", MultinomialNB()),\n",
    "    (\"Logistic Regression\", LogisticRegression()),\n",
    "    (\"RandomForestClassifier\", RandomForestClassifier())    \n",
    "]\n",
    "\n",
    "best_model_name = None\n",
    "best_model_accuracy = 0\n",
    "\n",
    "# Train and evaluate the models on the train and validation sets\n",
    "for name, model in models:\n",
    "    model.fit(X_train, y_train)\n",
    "    y_val_pred = model.predict(X_val)\n",
    "    accuracy = accuracy_score(y_true=y_val, y_pred=y_val_pred)\n",
    "    print(f\"{name}: Validation Accuracy = {accuracy:.4f}\")\n",
    "    \n",
    "    if accuracy > best_model_accuracy:\n",
    "        best_model_accuracy = accuracy\n",
    "        best_model_name = name\n",
    "\n",
    "print(f\"Best Model: {best_model_name} with Validation Accuracy = {best_model_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a49a8943",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, apply the best model on test data and make predictions\n",
    "\n",
    "best_model = None\n",
    "\n",
    "# Subset the best model from models list\n",
    "for name, model in models:\n",
    "    if model == best_model_name:\n",
    "        model = best_model\n",
    "    \n",
    "best_model = model\n",
    "best_model_fit = best_model.fit(X_train_tfidf, X['vertical'])\n",
    "y['vertical'] = best_model_fit.predict(y_test_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "945c27c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>keywords</th>\n",
       "      <th>vertical</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>watch,series,online,watchseries,watch,series,v...</td>\n",
       "      <td>tv-movies-and-streaming</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>anime,online,anime,online,sub,español,anime,on...</td>\n",
       "      <td>sports</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>football,stream,nfl,stream,soccer,stream,tenni...</td>\n",
       "      <td>sports</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>football,footem,site,footem7,footem,site,epics...</td>\n",
       "      <td>sports</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>online,storage,free,storage,cloud,storage,coll...</td>\n",
       "      <td>file-sharing-and-hosting</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>your,videos,easily,higher,advertisingrevenues,...</td>\n",
       "      <td>tv-movies-and-streaming</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            keywords                  vertical\n",
       "0  watch,series,online,watchseries,watch,series,v...   tv-movies-and-streaming\n",
       "1  anime,online,anime,online,sub,español,anime,on...                    sports\n",
       "2  football,stream,nfl,stream,soccer,stream,tenni...                    sports\n",
       "3  football,footem,site,footem7,footem,site,epics...                    sports\n",
       "4  online,storage,free,storage,cloud,storage,coll...  file-sharing-and-hosting\n",
       "5  your,videos,easily,higher,advertisingrevenues,...   tv-movies-and-streaming"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.to_csv(\"RandomForestClassifier_predictions.csv\", header=True)\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef0f03e5",
   "metadata": {},
   "source": [
    "Although the RandomForestClassifier has the highest score among these models (~0.83) compared to the better performing but with significantly lower score of 0.62 MultinomialNB. There might be several reasons for this:\n",
    "- Model overfitting - learning specific details and noise in the training data, which results in overly optimistic performance on test data but poor performance on previously unseen data. \n",
    "- Data imbalance - for some models, if the number of sample of one class is greater then the number of samples of the remaining classes, the model can favor the majority class and struggle to predict the minority classes.\n",
    "\n",
    "Data imbalance can be addressed by checking the class distribution. Inspecting the class distriution, we can see that it is uniform accross the scraped (traninig) dataset, because each of the 3 classes are represented by 50 instances. Each algorithm is trained using exactly 50 instances of each class, so data imbalance is unlikely to cause poor model performance.\n",
    "\n",
    "We can also use cross validation by splitting the data into several folds, train the model the hold-out set and then evaluate its performance on each fold. This would provide better assessment of the model's generalization capabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e256b4e5",
   "metadata": {},
   "source": [
    "### Building a machine learning pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7b4684d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "text_clf = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', MultinomialNB()),\n",
    "])\n",
    "\n",
    "text_clf.fit(X['keywords'], X['vertical'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2df70df",
   "metadata": {},
   "source": [
    "New ML pipeline with model selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e6239a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Create a pipeline with data preprocessing and best model selection\n",
    "pipeline = Pipeline([\n",
    "    ('vect', TfidfVectorizer()),\n",
    "    ('model', best_model)\n",
    "])\n",
    "\n",
    "# Fit the pipeline on the combined train and validation sets\n",
    "pipeline.fit(X['keywords'], X['vertical'])\n",
    "\n",
    "# Apply the trained pipeline on the test data and make predictions\n",
    "y['vertical'] = pipeline.predict(y['keywords'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f6acaa9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>keywords</th>\n",
       "      <th>vertical</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>watch,series,online,watchseries,watch,series,v...</td>\n",
       "      <td>tv-movies-and-streaming</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>anime,online,anime,online,sub,español,anime,on...</td>\n",
       "      <td>sports</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>football,stream,nfl,stream,soccer,stream,tenni...</td>\n",
       "      <td>sports</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>football,footem,site,footem7,footem,site,epics...</td>\n",
       "      <td>sports</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>online,storage,free,storage,cloud,storage,coll...</td>\n",
       "      <td>file-sharing-and-hosting</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>your,videos,easily,higher,advertisingrevenues,...</td>\n",
       "      <td>tv-movies-and-streaming</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            keywords                  vertical\n",
       "0  watch,series,online,watchseries,watch,series,v...   tv-movies-and-streaming\n",
       "1  anime,online,anime,online,sub,español,anime,on...                    sports\n",
       "2  football,stream,nfl,stream,soccer,stream,tenni...                    sports\n",
       "3  football,footem,site,footem7,footem,site,epics...                    sports\n",
       "4  online,storage,free,storage,cloud,storage,coll...  file-sharing-and-hosting\n",
       "5  your,videos,easily,higher,advertisingrevenues,...   tv-movies-and-streaming"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f513d6e",
   "metadata": {},
   "source": [
    "### Evaluation of model performance on the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3c7a7086",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "docs_test = y['keywords']\n",
    "predicted = pipeline.predict(docs_test)\n",
    "np.mean(predicted == y['vertical'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2a429256",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                          precision    recall  f1-score   support\n",
      "\n",
      "                  sports       1.00      1.00      1.00         1\n",
      " tv-movies-and-streaming       1.00      1.00      1.00         3\n",
      "file-sharing-and-hosting       1.00      1.00      1.00         2\n",
      "\n",
      "                accuracy                           1.00         6\n",
      "               macro avg       1.00      1.00      1.00         6\n",
      "            weighted avg       1.00      1.00      1.00         6\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "target_names = X['vertical'].unique()\n",
    "target_names\n",
    "print(metrics.classification_report(y['vertical'], predicted, target_names=target_names))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
